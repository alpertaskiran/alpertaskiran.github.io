[{"content":"#WIP #TODO:images,formating github repo: https://github.com/alpertaskiran/bayesian_mmm\nModel This work is based on Jin, Yuxue, et al (2017) in which they provided a Bayesian mixed media model with carryover and shape effect. As it is described in the case study I avoided modeling ad stock shape effects with saturation or diminishing returns. For the sake of completeness, I will define the problem:\nTime series data with $y$ target variable (revenue), channel spending $x$ and control variables $z$ captures trend and seasonality. $t = 1 \\dots T$ . There are $M$ media channels in the media mix, and $x_{t,m}$ is the media spend of channel $m$ at week $t$ It is considered a linear model of the form where $\\tau$ is the intercept capturing baseline sales, and $\\epsilon$ capturing noise is assumed to be uncorrelated with other variables and have constant variance. $f$ is the ad stock function: ![[Pasted image 20230417093241.png|center]] Ad stock function takes as input media spends for a given media during L weeks, the retain-rate and the delay of the peak. The delay is the number of periods before the peak effect. The ad stock function is responsible for capturing the temporal effects of diverse advertising channels. $\\alpha_{m}$ representing the retention rate of ad effect and $\\theta_{m}$ represents the delay in the peak effect of the $m$-th channel. $L$ is the maximum time period that delay can incur effect and it is set to be 13.\nSeasonality and Trend Employing Seasonal-Trend decomposition using LOWESS (locally estimated weighted scatterplot smoothing). For log of target value and setting seasonal to 7 results in lowest residual. This supports the need of additive modeling of seasonality and trend. The idea is to make a matrix of Fourier features which get multiplied by a vector of coefficients to to capture the seasonality. Number of order to create fourier pairs will be 7 (14 new features). Trend will be modeled as linear function. This is further supported by a periodogram where drop after bi-monthly can be seen from the figure below.\nLikelihood and Prior distributions The prior distribution represents the beliefs or assumptions you have about the variables before analyzing the data. Starting point for the choice of priors is following the work done by Jin, Yuxue, et al (2017). Parameter optimization is avoided but one can further it using Bayesian model selection criteria.\nSelecting the likelihood function: The likelihood function describes the relationship between the data and the parameters of the model. Choosing an appropriate likelihood function is critical to accurately model the data. $\\alpha \\backsim Beta(3,3)$, $\\theta \\backsim Uniform(0,12)$, $\\gamma \\backsim Laplace(0,1)$ for the fourier coefficient to add certain regularization, $\\tau \\backsim Normal(0,1)$ additionally trend has a coefficient with $~Normal(0,1)$\nI use a $HalfNormal(1)$ distribution for the media coefficients to ensure they are positive. Main model I used for the likelihood function is a StudentT distribution which is robust against outliers as suggested by this[4] with the precision prior parameter $\\nu \\backsim Gamme(15,1)$ It is suggested by [4]] to use MaxAbsScaler which is convenient to use due to transformation while calculation ROAS.\nModel validation Prior predictive checks make use of simulations from the model. Range is limited to but suggesting negative revenue especially in the initial points should be investigated further. Additional data to support initiation could be effective.\nIt is important to validate the model to ensure that it is reliable and accurate. This can be done by by using posterior predictive checks. Posterior predictive checking also allows one to examine the fit of a model to real data. We see that some extreme values can not be captured.\nChannel performance Channel 3 , Channel 7, Channel 6 contributions are highest to revenue. It is followed by Channel 4, Channel 5. Lowest contributions are from Channel 2, Channel 1 (see notebook) Further we observe that delay parameters have high HDI range and fail to converge. This could further improved by using saturation function. We also observe that the Channel 3 (coeff =0.376), Channel 7 (coeff = 320) and Channel 6 (coeff =305 ) has highest average contribution by unit spending. It is followed by Channel 2( coeff = (0.225))\nModel Comparison We can check the difference with respect to using normal distributed likelihood. To evaluate model performance and to measure it we will use Pareto smoothed importance sampling leave-one-out cross-validation (LOO). We see that our initial model performs better.\nFurther diagnostics can be applied by checking residuals of the model. For each MCMC draw of posterior samples of the parameters we should compute autocorrelation of the residuals of the regression model as suggested by[1] Since this is our main model assumption.\nReferences: Jin, Yuxue, et al. “Bayesian methods for media mix modeling with carryover and shape effects.” (2017)\nGelman, Andrew, et al. “Bayesian Workflow.” ArXiv.org, 3 Nov. 2020\nPyMC-Marketing : https://github.com/pymc-labs/pymc-marketing\nMedia effect estimation with pymc: Adstock, saturation \u0026amp; diminishing returns. Dr. Juan Camilo Orduz. (2022, February 11). Retrieved April 17, 2023, from https://juanitorduz.github.io/pymc_mmm/\n","permalink":"https://alpertaskiran.github.io/posts/mmm/","summary":"#WIP #TODO:images,formating github repo: https://github.com/alpertaskiran/bayesian_mmm\nModel This work is based on Jin, Yuxue, et al (2017) in which they provided a Bayesian mixed media model with carryover and shape effect. As it is described in the case study I avoided modeling ad stock shape effects with saturation or diminishing returns. For the sake of completeness, I will define the problem:\nTime series data with $y$ target variable (revenue), channel spending $x$ and control variables $z$ captures trend and seasonality.","title":"Bayesian Mix Media Model"},{"content":"#WIP\nrepo:https://python.langchain.com/en/latest/index.html\n","permalink":"https://alpertaskiran.github.io/posts/llm/","summary":"#WIP\nrepo:https://python.langchain.com/en/latest/index.html","title":"LLM's Langchain and Prompt engineering"},{"content":"Welcome to my blog! My name is Alper and I am a passionate learner and practitioner in the field of machine learning and artificial intelligence. I have always been fascinated by the potential of these cutting-edge technologies to transform the world we live in, and have spent countless hours studying and experimenting with different ML algorithms and applications. Through this blog, I hope to share my learnings and findings with like-minded individuals who are equally passionate about exploring the potential of ML and AI.\nAs we continue to develop more sophisticated AI systems, it is becoming increasingly important to ensure that these systems are designed with human needs and values in mind. By investigating the intersection of operations research, machine learning, and algorithmic game theory, I hope to contribute to the development of more effective and ethical AI systems that can improve our lives in meaningful ways.\nThrough this blog, I hope to share my experiences and insights with fellow learners and enthusiasts who are interested in exploring the fascinating world of machine learning and artificial intelligence.So join me as we delve into the exciting world of ML and AI, exploring the intersection of theory and technology along the way.\nAnd always feel free to share your thoughts at: alpertaskiran@protonmail.com\nLet\u0026rsquo;s start!\nPS: yes! There will be mathematics! $\\displaystyle e^{i\\pi}+1=0$\n","permalink":"https://alpertaskiran.github.io/posts/first_post/","summary":"Welcome to my blog! My name is Alper and I am a passionate learner and practitioner in the field of machine learning and artificial intelligence. I have always been fascinated by the potential of these cutting-edge technologies to transform the world we live in, and have spent countless hours studying and experimenting with different ML algorithms and applications. Through this blog, I hope to share my learnings and findings with like-minded individuals who are equally passionate about exploring the potential of ML and AI.","title":"First!"}]